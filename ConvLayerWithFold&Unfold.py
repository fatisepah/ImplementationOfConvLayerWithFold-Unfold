# -*- coding: utf-8 -*-
"""unfold.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sY3RA-GTJ8kMGzLnW8FvcStuu_Lstb5i
"""

import torch
import numpy as np

b=0
inp1 = [[b+1 for j in range(3)]for i in range(2)]

inp1

inp2=np.arange(1,121)

inp3=inp2.reshape(1,1,10,12)
inp=torch.tensor(inp3)

inp.shape

inp

#inp = torch.randn(1, 2, 5, 5)
#w = torch.randn(2, 1, 3, 3)

#inp = torch.tensor([[[[1, 2, 3, 4, 5],
#        [6, 7, 8, 9, 10],
#        [11,12,13,14,15],
#        [16,17,18,19,20],
#        [21,22,23,24,25]],
#
#        [[1, 2, 3, 4, 5],
#        [6, 7, 8, 9, 10],
#        [11,12,13,14,15],
#        [16,17,18,19,20],
#        [21,22,23,24,25]]]])

#inp = torch.tensor([[[[1, 2, 3, 4, 5],
#        [6, 7, 8, 9, 10],
#        [11,12,13,14,15],
#        [16,17,18,19,20],
#        [21,22,23,24,25]]]])

inp.shape

#w = torch.tensor([[[[31, 32, 33],
#        [34, 35, 36],
#        [37,38,39]]],
#
#        [[[40, 41, 42],
#        [43, 44, 45],
#        [46,47,48]]]])

#w = torch.tensor([[[[31, 32, 33],
#        [34, 35, 36],
#        [37,38,39]]]])

inp.shape, w.shape

inp_unf = torch.nn.functional.unfold(inp, (3, 3))

inp_unf.shape

w.size(0)

w.view(w.size(0), -1).t().shape

inp_unf.transpose(1, 2).shape

out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)

out_unf.shape

out = out_unf.view(1, 2, 7, 8)
out.shape

(torch.nn.functional.conv2d(inp, w) - out).abs().max()

import torch
from torch import nn, optim
import torch.nn.functional as F

in_channels = 2
out_channels = 5
size = 4
torch.manual_seed(123)
X = torch.rand(1, in_channels, size, size)

conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False)
out = conv(X)
print('out', out)
print('out.size()', out.size())

print('')
Xunfold = F.unfold(X, kernel_size=3, padding=1)

Xunfold.shape

print('X.size()', X.size())
print('Xunfold.size()', Xunfold.size()
kernels_flat = conv.weight.data.view(out_channels, -1)
print('kernels_flat.size()', kernels_flat.size()
res = kernels_flat @ Xunfold
res = res.view(1, out_channels, size, size)
print('res', res)
print('res.size()', res.size())

inp2=np.arange(1,121)
inp3=inp2.reshape(1,1,10,12)
inprr=torch.tensor(inp3)

w2=np.arange(1,26)
w3=w2.reshape(1,1,5,5)
wrr=torch.tensor(w3)

# Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)
inp = torch.randn(1, 1, 10, 12)
w = torch.randn(1, 1, 5, 5)

inp.shape,w.shape

inp,w

inp_unf = torch.nn.functional.unfold(inp, (5, 5))

inp_unf.shape

inp_unf[0,0,:]

inp_unf.shape

out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)

out = out_unf.view(1, 2, 7, 8)

(torch.nn.functional.conv2d(inp, w) - out).abs().max()

#توابع جدیدی که برای فولد و آنفولد نوشتم

import torch

inp=torch.ones(1,3,6,6)
inp[0,1,:,:]=2

#w=torch.ones(2,3,2,2)
w=torch.tensor([[[[1,2],
                  [3,4]],
                 [[5,6],
                  [7,8]],
                 [[9,10],
                 [11,12]]],
                [[[13,14],
                  [15,16]],
                 [[17,18],
                  [19,20]],
                 [[21,22],
                 [23,24]]]])

w=w.float()

w.shape

unfolded=torch.nn.functional.unfold(inp,(2,2))

unfolded.shape

inp.shape#,inp

unfolded.shape#,unfolded

l=unfolded.transpose(1,2)
l.shape#,l

w.size(0),w.size(1),w.size(2),w.size(3)

w.view(w.size(1),-1)

w,w.view(w.size(0),-1),w.view(w.size(0),-1).t()

w.shape,w.view(w.size(0),-1).shape,w.view(w.size(0),-1).t().shape

unfolded.transpose(1,2).shape,w.view(w.size(0),-1).t().shape

unfolded.transpose(1,2).matmul(w.view(w.size(0),-1).t()).shape

conv_output=unfolded.transpose(1,2).matmul(w.view(w.size(0),-1).t()).transpose(1,2)

conv_output.shape

out=torch.nn.functional.fold(conv_output,(5,5),(1,1))

out.shape

#implementation conv layer with fold and unfold

inputconv1=torch.randn(1,1,28,28)
weightconv1=torch.randn(6,1,5,5)

unfoldedconv1=torch.nn.functional.unfold(inputconv1,(5,5))

unfoldedconv1.shape

unfoldedconv1.transpose(1,2).shape

(weightconv1.view(weightconv1.size(0),-1).t()).shape

conv1_output=unfoldedconv1.transpose(1,2).matmul(weightconv1.view(weightconv1.size(0),-1).t()).transpose(1,2)

conv1_output.shape

out=torch.nn.functional.fold(conv1_output,(24,24),(1,1))

out.shape

(torch.nn.functional.conv2d(inputconv1,weightconv1)-out).abs()